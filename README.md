# frontier-evaluations
We're building PhD-level question sets to probe genuine expert reasoning in LLMs, starting with Neuroscience & Brain-Computer Interfaces.
This v1.0 set (500 questions) was generated synthetically but designed for deep multi-step synthesis across subfields (motor cortex plasticity, ion-trap gates, manifold dynamics, etc.).
We're releasing it fully public and openly seeking feedback/validation from domain experts. Does it capture real frontier difficulty? Spot any issues/traps that need tuning?
Download the JSONL below. Run your models and share results!
[neuroscience_bci_phd_evals_v1.0.JSONL]
More domains coming soon.

# Update 4:49EST February 2, 2026
After running the eval for the first time, frontier model scores were over 80% consistently. 

Without frontier rigor, these evals are toothless, so we are iterating on v1.1.

It will be harder, and we will be back with it soon!
